{
    "name": "mistake_detect_bert_description",
    "model": {
        "bert-model": true,
        "vocab_size": 40000,
        "emb_size": 200,
        "bert_max_len":128
    },
    "training": {
        "optimizer": {
            "type": "Adam",
            "params": {
                "lr": 0.004,
                "amsgrad": true
            },
            "scheduler": {
                "type": "CyclicLR",
                "params": {
                    "base_lr": 0.0001,
                    "max_lr": 0.004,
                    "step_size": 5,
                    "mode": "triangular"
                }
            }
        },
        "epochs": 3,
        "train_batch_size": 16,
	    "test_batch_size": 16,
        "num_workers": 0,
        "gradient_clipping": {
            "use": false,
            "clip_value": 1.0
        },
        "shuffle": true,
        "warmup_proportion": 0,
        "learning_rate": 1e-5,
        "decay_rate": 0.99,
        "decay_step": 100000,
        "total_training_steps": 500000
    },
    "system": {
        "device": "cuda",
        "num_workers": 1,
        "base_dir": "/home/zhaochen/postdoc/tim-reasoning-models/mistake-detect-bert-description/",
	    "train_data": "/home/zhaochen/postdoc/tim-reasoning/data/description_data_train.json",
        "test_data": "/home/zhaochen/postdoc/tim-reasoning/data/description_data_test.json"
    },
    "bert_token_file": "bert-base-uncased",
    "bert_model_file": "bert-base-uncased",
    "bert_model_config": {
        "attention_probs_dropout_prob": 0.1, 
  	"directionality": "bidi", 
  	"hidden_act": "gelu", 
  	"hidden_dropout_prob": 0.1, 
 	"hidden_size": 768, 
  	"initializer_range": 0.02, 
  	"intermediate_size": 3072, 
  	"max_position_embeddings": 512, 
  	"num_attention_heads": 12, 
  	"num_hidden_layers": 12, 
  	"pooler_fc_size": 768, 
  	"pooler_num_attention_heads": 12, 
  	"pooler_num_fc_layers": 3, 
  	"pooler_size_per_head": 128, 
    "pooler_type": "first_token_transform", 
    "checkpoint": 10
      
    }
}
