{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the NER Model with spaCy v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.tokens import DocBin\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the annotated data into the spaCy bin object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentence(sentence_content, sentence_counter):\n",
    "    #print('Processing sentence: %d' % sentence_counter)\n",
    "    index = 0\n",
    "    entities = []\n",
    "    sentence_text = ''\n",
    "    for token, tag in sentence_content:\n",
    "        if tag in {'B-COMMENT', 'I-COMMENT', 'OTHER'}:  # Skip these tags\n",
    "            tag = 'O'\n",
    "        if tag in {'B-RANGE_END', 'B-INDEX'}:\n",
    "            tag = 'QTY'\n",
    "        tag = tag.replace('B-', '')\n",
    "        tag = tag.replace('I-', '')\n",
    "        tag = tag.replace('NAME', 'INGREDIENT')\n",
    "        tag = tag.replace('QTY', 'QUANTITY')\n",
    "        \n",
    "        if tag != 'O':\n",
    "            entity_info = [index, index+len(token), tag]\n",
    "            entities.append(entity_info)\n",
    "        sentence_text += token + ' '\n",
    "        index += len(token)+1  # Plus 1 becuase of the empty space\n",
    "    \n",
    "    sentence_data = [sentence_text.rstrip(), {'entities': entities}]\n",
    "\n",
    "    return sentence_data\n",
    "\n",
    "def read_nyt_dataset():\n",
    "    dataset = []\n",
    "    sentence_content = []\n",
    "    sentence_counter = 1\n",
    "    \n",
    "    with open('resource/dataset_nyt') as fin:\n",
    "        file_lines = fin.readlines()\n",
    "    \n",
    "    for file_line in file_lines:\n",
    "        file_line = file_line.strip()\n",
    "\n",
    "        if len(file_line) > 0:  \n",
    "            items = file_line.split('\\t')\n",
    "            token, tag = items[0], items[5]\n",
    "            sentence_content.append((token, tag))\n",
    "        else:# End of the phrase\n",
    "            sentence_data = process_sentence(sentence_content, sentence_counter)\n",
    "            if len (sentence_data[1]['entities']) > 0:\n",
    "                dataset.append(sentence_data)\n",
    "            sentence_content = []\n",
    "            sentence_counter += 1\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def build_bin_object(dataset, set_name):\n",
    "    nlp = spacy.blank('en') # load a new spacy model\n",
    "    db = DocBin() # create a DocBin object\n",
    "    for text, annot in tqdm(dataset): # data in previous format\n",
    "        doc = nlp.make_doc(text) # create doc object from text\n",
    "        ents = []\n",
    "        for start, end, label in annot['entities']: # add character indexes\n",
    "            span = doc.char_span(start, end, label=label, alignment_mode='contract')\n",
    "            if span is None:\n",
    "                print('Skipping entity')\n",
    "            else:\n",
    "                ents.append(span)\n",
    "        try:\n",
    "            doc.ents = ents # label the text with the ents\n",
    "            db.add(doc)\n",
    "        except:\n",
    "            print(text, annot)\n",
    "    db.to_disk('resource/%s_data.spacy' % set_name) # save the docbin object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 117140/117140 [00:55<00:00, 2104.50it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50204/50204 [00:23<00:00, 2108.39it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = read_nyt_dataset()\n",
    "split_index = int(len(dataset) * 0.70)  # Use 70% and 30% splits\n",
    "train_data = dataset[:split_index]\n",
    "dev_data = dataset[split_index:]\n",
    "build_bin_object(train_data, 'train')\n",
    "build_bin_object(dev_data, 'dev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the config file to train via command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the base_config.cfg file from here: https://spacy.io/usage/training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\r\n",
      "\u001b[38;5;2m✔ Saved config\u001b[0m\r\n",
      "config.cfg\r\n",
      "You can now add your data and train your pipeline:\r\n",
      "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\r\n"
     ]
    }
   ],
   "source": [
    "! python -m spacy init fill-config base_config.cfg config.cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model using the command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Saving to output directory: resource/model_ner\u001b[0m\n",
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[2022-06-30 14:16:23,956] [INFO] Set up nlp object from config\n",
      "[2022-06-30 14:16:23,967] [INFO] Pipeline: ['tok2vec', 'ner']\n",
      "[2022-06-30 14:16:23,971] [INFO] Created vocabulary\n",
      "[2022-06-30 14:16:23,972] [INFO] Finished initializing nlp object\n",
      "[2022-06-30 14:17:52,616] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
      "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  --------  ------  ------  ------  ------\n",
      "  0       0          0.00     71.00   15.88   12.18   22.81    0.16\n",
      "  0      10          0.62    806.83   31.16   84.77   19.09    0.31\n",
      "  0      20          1.91    530.29   39.61   84.38   25.88    0.40\n",
      "  0      30          3.58    402.93   72.05   66.59   78.49    0.72\n",
      "  0      40          7.28    277.56   75.65   74.65   76.68    0.76\n",
      "  0      50         10.17    213.76   79.23   76.83   81.79    0.79\n",
      "  0      60         10.81    231.96   78.25   85.09   72.42    0.78\n",
      "  0      70          6.77    164.95   79.15   84.30   74.60    0.79\n",
      "  0      80          8.41    151.74   81.26   84.66   78.12    0.81\n",
      "  0      90          9.45    148.95   82.30   84.98   79.77    0.82\n",
      "  0     100         10.07    155.59   82.61   86.74   78.86    0.83\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "resource/model_ner/model-last\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train config.cfg --output ./resource/model_ner/ --paths.train ./resource/train_data.spacy --paths.dev ./resource/dev_data.spacy --training.eval_frequency 10 --training.max_steps 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('resource/model_ner/model-last/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {'QUANTITY': 'yellow', 'UNIT': 'green', 'INGREDIENT': 'orange'}\n",
    "options = {'ents': ['QUANTITY', 'UNIT', 'INGREDIENT'], 'colors':colors}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: yellow; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">QUANTITY</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: green; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    cups\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">UNIT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: orange; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    cherry\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">INGREDIENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: orange; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    tomatoes\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">INGREDIENT</span>\n",
       "</mark>\n",
       ", sliced into quarters</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp('2 cups cherry tomatoes, sliced into quarters')\n",
    "displacy.render(doc, style='ent', jupyter=True, options=options)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
