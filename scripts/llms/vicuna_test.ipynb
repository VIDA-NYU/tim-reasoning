{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c488b465",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/py38/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "#login(token=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135f7465",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "snapshot_download(repo_id=\"lmsys/vicuna-13b-v1.5\", local_dir=\"./data/vicuna/13b\", cache_dir=\"/home/rl3725/scratch/cache_huggingface\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7c5fde",
   "metadata": {},
   "source": [
    "## 1. Zero-Shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3faa2002",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:37<00:00, 12.66s/it]\n",
      "/ext3/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/ext3/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"./data/vicuna/13b\", use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./data/vicuna/13b\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e931aa0-bb3f-4c64-98f7-ce015f8231c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*** Generate:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/utils.py:1411: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n",
      "/ext3/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> A chat between a user and a reasoning system. You are a reasoning system that has to identify in which step of a recipe the user is on given some inputs from a perception system. The reasoning system gives helpful, detailed, and polite answers to the user's questions.\n",
      "\n",
      "USER: Can you help me to identify in which step of a recipe the user is?\n",
      "ASSISTANT:\n",
      "\n",
      "Sure, I'd be happy to help you identify the step of a recipe that the user is on. To do this, I'll need some information from you. Please provide me with the following details:\n",
      "\n",
      "1. The recipe name and the type of dish it is.\n",
      "2. The ingredients used in the recipe.\n",
      "3. The instructions or steps provided by the recipe.\n",
      "4. The current state of the dish or any recent actions performed by the user.\n",
      "\n",
      "With this information, I can analyze the recipe and help you determine the step of the recipe that the user is currently on.</s>\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Can you help me to identify in which step of a recipe the user is?\"\n",
    "prompt_template=f\"\"\"A chat between a user and a reasoning system. You are a reasoning system that has to identify in which step of a recipe the user is on given some inputs from a perception system. The reasoning system gives helpful, detailed, and polite answers to the user's questions.\n",
    "\n",
    "USER: {prompt}\n",
    "ASSISTANT:\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\\n*** Generate:\")\n",
    "\n",
    "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79c5e0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "<s> \n",
      "USER:\n",
      "It's an oatmeal recipe and these are the steps:\n",
      "1: Measure 4 Tablespoons of quick-cook oats and place in a microwave-safe bowl.,\n",
      "2: Add 1/2 cup water to the bowl.,\n",
      "3: Microwave for 30 seconds.,\n",
      "4: Add handful of raisins and stir.,\n",
      "5: Microwave for 60 more seconds then stir.,\n",
      "6: Top with banana slices.,\n",
      "7: Sprinkle with cinnamon and drizzle with honey before serving.\n",
      "\n",
      "And these are the possible states detected by the perception system and their probabilities:\n",
      "oatmeal with water:0.54,\n",
      "oatmeal with raisins with banana with cinnamon:0.06,\n",
      "oatmeal with raisins:0.13,\n",
      "oatmeal with raisins with banana:0.01,\n",
      "oatmeal[dry]:0.15,\n",
      "oatmeal with raisins with banana with cinnamon with honey:0.06,\n",
      "cooked oatmeal:0.00,\n",
      "empty:0.01\n",
      "So, in which step of the recipe the user is?\n",
      "ASSISTANT:\n",
      "Based on the probabilities provided, the user is most likely in Step 5 of the recipe, which is microwaving the oatmeal for an additional 60 seconds after adding the raisins. The probability of the state \"oatmeal with raisins with banana with cinnamon\" is 0.06, which is the highest probability among the possible states detected by the perception system. This state corresponds to Step 7 of the recipe, which is topping the oatmeal with banana slices, sprinkling with cinnamon, and drizzling with honey before serving. However, since the probability of this state is relatively low, it is possible that the user has not yet reached this step.</s>\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "USER:\n",
    "It's an oatmeal recipe and these are the steps:\n",
    "1: Measure 4 Tablespoons of quick-cook oats and place in a microwave-safe bowl.,\n",
    "2: Add 1/2 cup water to the bowl.,\n",
    "3: Microwave for 30 seconds.,\n",
    "4: Add handful of raisins and stir.,\n",
    "5: Microwave for 60 more seconds then stir.,\n",
    "6: Top with banana slices.,\n",
    "7: Sprinkle with cinnamon and drizzle with honey before serving.\n",
    "\n",
    "And these are the possible states detected by the perception system and their probabilities:\n",
    "oatmeal with water:0.54,\n",
    "oatmeal with raisins with banana with cinnamon:0.06,\n",
    "oatmeal with raisins:0.13,\n",
    "oatmeal with raisins with banana:0.01,\n",
    "oatmeal[dry]:0.15,\n",
    "oatmeal with raisins with banana with cinnamon with honey:0.06,\n",
    "cooked oatmeal:0.00,\n",
    "empty:0.01\n",
    "So, in which step of the recipe the user is?\n",
    "ASSISTANT:\n",
    "\"\"\"\n",
    "prompt_template=f'{prompt}'\n",
    "\n",
    "print(\"Output:\")\n",
    "\n",
    "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6163c3b8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2. Few-Shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fc7f49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def read_json(file_paths):\n",
    "    data = []\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, 'r') as file:\n",
    "            recipe_data = json.load(file)\n",
    "            data.extend(recipe_data)\n",
    "    return data\n",
    "\n",
    "def seconds_to_minutes(seconds):\n",
    "    minutes = seconds // 60\n",
    "    remaining_seconds = seconds % 60\n",
    "\n",
    "    return minutes, remaining_seconds\n",
    "\n",
    "def two_digit_number(number):\n",
    "    return f'{number:02}'\n",
    "\n",
    "def make_hr_state(state):\n",
    "    new_state = state.replace('+', ' with ')\n",
    "    new_state = new_state.replace('[', '(')\n",
    "    new_state = new_state.replace(']', ')')\n",
    "    \n",
    "    return new_state\n",
    "\n",
    "def extract_data(data, include_step=True):\n",
    "    dataset = []\n",
    "    times = set()\n",
    "    for entry in data:\n",
    "        raw_seconds = int(entry['timestamp'])\n",
    "        if raw_seconds in times:\n",
    "            continue\n",
    "        else:\n",
    "            times.add(raw_seconds)\n",
    "        minutes, seconds = seconds_to_minutes(raw_seconds)\n",
    "        image_data_list = entry['objects']\n",
    "        step = entry['step']\n",
    "        \n",
    "        for image_data in image_data_list:\n",
    "            object = image_data['label']\n",
    "            if 'state' in image_data:\n",
    "                states = [f'{make_hr_state(s)}: {round(v,2)}' for s,v in image_data['state'].items()]\n",
    "                text = f'At 01:{two_digit_number(minutes)}:{two_digit_number(seconds)} PM, the possible object states for {object} are: {\", \".join(states)}. The predicted step is:'\n",
    "                if include_step:\n",
    "                    text = text + f' {step}'\n",
    "                dataset.append(text)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45e8dbd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:37<00:00, 12.50s/it]\n",
      "/ext3/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/ext3/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"./data/vicuna/13b\", use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./data/vicuna/13b\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04852bd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['At 01:00:14 PM, the possible object states for mug are: dripper with filter with coffee(dry): 0.47, dripper with filter: 0.3, tea-bag: 0.05, tea-bag with water: 0.15, dripper with filter with coffee(wet): 0.05, dripper(empty): 0.05, empty: 0.09. The predicted step is: 1',\n",
       " 'At 01:00:15 PM, the possible object states for mug are: dripper with filter with coffee(dry): 0.61, dripper with filter: 0.01, tea-bag: 0.02, tea-bag with water: 0.06, dripper with filter with coffee(wet): 0.04, dripper(empty): 0.11, empty: 0.09, water: 0.06. The predicted step is: 1',\n",
       " 'At 01:00:15 PM, the possible object states for bowl are: oatmeal(cooked): 0.55, oatmeal(dry): 0.45. The predicted step is: 1',\n",
       " 'At 01:00:16 PM, the possible object states for mug are: dripper with filter with coffee(dry): 0.52, dripper with filter: 0.07, tea-bag: 0.12, tea-bag with water: 0.03, dripper with filter with coffee(wet): 0.07, dripper(empty): 0.06, empty: 0.05, water: 0.04, dripper with filter with coffee(drained): 0.18. The predicted step is: 1',\n",
       " 'At 01:00:16 PM, the possible object states for bowl are: oatmeal(cooked): 0.58, oatmeal(dry): 0.26, oatmeal with water: 0.11, oatmeal with raisins with banana: 0.08. The predicted step is: 1',\n",
       " 'At 01:00:17 PM, the possible object states for mug are: dripper with filter with coffee(dry): 0.7, dripper with filter: 0.09, tea-bag: 0.01, tea-bag with water: 0.13, dripper with filter with coffee(wet): 0.03, dripper(empty): 0.0, empty: 0.0, water: 0.03, dripper with filter with coffee(drained): 0.0. The predicted step is: 1',\n",
       " 'At 01:00:17 PM, the possible object states for bowl are: oatmeal(cooked): 0.22, oatmeal(dry): 0.37, oatmeal with water: 0.09, oatmeal with raisins with banana: 0.05, oatmeal with raisins: 0.28, oatmeal with raisins with banana with cinnamon with honey: 0.05, empty: 0.07. The predicted step is: 1',\n",
       " 'At 01:00:18 PM, the possible object states for mug are: dripper with filter with coffee(dry): 0.37, dripper with filter: 0.16, tea-bag: 0.05, tea-bag with water: 0.08, dripper with filter with coffee(wet): 0.17, dripper(empty): 0.09, empty: 0.08, water: 0.01, dripper with filter with coffee(drained): 0.0. The predicted step is: 1',\n",
       " 'At 01:00:18 PM, the possible object states for bowl are: oatmeal(cooked): 0.18, oatmeal(dry): 0.11, oatmeal with water: 0.71, oatmeal with raisins with banana: 0.0, oatmeal with raisins: 0.0, oatmeal with raisins with banana with cinnamon with honey: 0.0, empty: 0.0. The predicted step is: 1',\n",
       " 'At 01:00:19 PM, the possible object states for mug are: dripper with filter with coffee(dry): 0.4, dripper with filter: 0.17, tea-bag: 0.13, tea-bag with water: 0.1, dripper with filter with coffee(wet): 0.09, dripper(empty): 0.01, empty: 0.01, water: 0.04, dripper with filter with coffee(drained): 0.05. The predicted step is: 1']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_paths = ['./perception_outputs/data_oatmeal_2023.11.03-18.51.03.json']\n",
    "data = read_json(file_paths)\n",
    "train_dataset = extract_data(data)\n",
    "train_dataset[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b92bc4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*** Generate:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/utils.py:1411: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n",
      "/ext3/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> A chat between a user and a reasoning system. You are a reasoning system that has to identify in which step of a recipe the user is on given some inputs from a perception system. The reasoning system gives helpful, detailed, and polite answers to the user's questions.\n",
      "\n",
      "USER: Can you help me to identify in which step of a recipe the user is?\n",
      "ASSISTANT:\n",
      "\n",
      "Sure, I'd be happy to help you identify the step of a recipe that the user is on. To do this, I'll need some information from you. Please provide me with the following details:\n",
      "\n",
      "1. The recipe name and the type of dish it is.\n",
      "2. The ingredients used in the recipe.\n",
      "3. The instructions or steps provided by the recipe.\n",
      "4. The current state of the dish or any recent actions performed by the user.\n",
      "\n",
      "With this information, I can analyze the recipe and help you determine the step of the recipe that the user is currently on.</s>\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Can you help me to identify in which step of a recipe the user is?\"\n",
    "prompt_template=f\"\"\"A chat between a user and a reasoning system. You are a reasoning system that has to identify in which step of a recipe the user is on given some inputs from a perception system. The reasoning system gives helpful, detailed, and polite answers to the user's questions.\n",
    "\n",
    "USER: {prompt}\n",
    "ASSISTANT:\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\\n*** Generate:\")\n",
    "\n",
    "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2678262d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "<s> \n",
      "USER:\n",
      "It's an oatmeal recipe and these are the steps:\n",
      "Step 1: Measure 4 Tablespoons of quick-cook oats and place in a microwave-safe bowl.\n",
      "Step 2: Add 1/2 cup water to the bowl.\n",
      "Step 3: Microwave for 30 seconds.\n",
      "Step 4: Add handful of raisins and stir.\n",
      "Step 5: Microwave for 60 more seconds then stir.\n",
      "Step 6: Top with banana slices.\n",
      "Step 7: Sprinkle with cinnamon and drizzle with honey before serving.\n",
      "\n",
      "Below you will see examples of object states identified by the perception system and the predicted steps for them.\n",
      "At 01:00:14 PM, the possible object states for mug are: dripper with filter with coffee(dry): 0.47, dripper with filter: 0.3, tea-bag: 0.05, tea-bag with water: 0.15, dripper with filter with coffee(wet): 0.05, dripper(empty): 0.05, empty: 0.09. The predicted step is: 1. At 01:00:15 PM, the possible object states for mug are: dripper with filter with coffee(dry): 0.61, dripper with filter: 0.01, tea-bag: 0.02, tea-bag with water: 0.06, dripper with filter with coffee(wet): 0.04, dripper(empty): 0.11, empty: 0.09, water: 0.06. The predicted step is: 1. At 01:00:15 PM, the possible object states for bowl are: oatmeal(cooked): 0.55, oatmeal(dry): 0.45. The predicted step is: 1. At 01:00:16 PM, the possible object states for mug are: dripper with filter with coffee(dry): 0.52, dripper with filter: 0.07, tea-bag: 0.12, tea-bag with water: 0.03, dripper with filter with coffee(wet): 0.07, dripper(empty): 0.06, empty: 0.05, water: 0.04, dripper with filter with coffee(drained): 0.18. The predicted step is: 1. At 01:00:16 PM, the possible object states for bowl are: oatmeal(cooked): 0.58, oatmeal(dry): 0.26, oatmeal with water: 0.11, oatmeal with raisins with banana: 0.08. The predicted step is: 1. At 01:00:17 PM, the possible object states for mug are: dripper with filter with coffee(dry): 0.7, dripper with filter: 0.09, tea-bag: 0.01, tea-bag with water: 0.13, dripper with filter with coffee(wet): 0.03, dripper(empty): 0.0, empty: 0.0, water: 0.03, dripper with filter with coffee(drained): 0.0. The predicted step is: 1. At 01:00:17 PM, the possible object states for bowl are: oatmeal(cooked): 0.22, oatmeal(dry): 0.37, oatmeal with water: 0.09, oatmeal with raisins with banana: 0.05, oatmeal with raisins: 0.28, oatmeal with raisins with banana with cinnamon with honey: 0.05, empty: 0.07. The predicted step is: 1. At 01:00:18 PM, the possible object states for mug are: dripper with filter with coffee(dry): 0.37, dripper with filter: 0.16, tea-bag: 0.05, tea-bag with water: 0.08, dripper with filter with coffee(wet): 0.17, dripper(empty): 0.09, empty: 0.08, water: 0.01, dripper with filter with coffee(drained): 0.0. The predicted step is: 1. At 01:00:18 PM, the possible object states for bowl are: oatmeal(cooked): 0.18, oatmeal(dry): 0.11, oatmeal with water: 0.71, oatmeal with raisins with banana: 0.0, oatmeal with raisins: 0.0, oatmeal with raisins with banana with cinnamon with honey: 0.0, empty: 0.0. The predicted step is: 1. At 01:00:19 PM, the possible object states for mug are: dripper with filter with coffee(dry): 0.4, dripper with filter: 0.17, tea-bag: 0.13, tea-bag with water: 0.1, dripper with filter with coffee(wet): 0.09, dripper(empty): 0.01, empty: 0.01, water: 0.04, dripper with filter with coffee(drained): 0.05. The predicted step is: 1\n",
      "Are you ready to predict which step of a recipe the user is on, given some object states identified by a perception system?\n",
      "ASSISTANT:\n",
      "Yes, I can help you with that. Please provide me with the object states identified by the perception system and the predicted step for each of the given time intervals.</s>\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "USER:\n",
    "It's an oatmeal recipe and these are the steps:\n",
    "Step 1: Measure 4 Tablespoons of quick-cook oats and place in a microwave-safe bowl.\n",
    "Step 2: Add 1/2 cup water to the bowl.\n",
    "Step 3: Microwave for 30 seconds.\n",
    "Step 4: Add handful of raisins and stir.\n",
    "Step 5: Microwave for 60 more seconds then stir.\n",
    "Step 6: Top with banana slices.\n",
    "Step 7: Sprinkle with cinnamon and drizzle with honey before serving.\n",
    "\n",
    "Below you will see examples of object states identified by the perception system and the predicted steps for them.\n",
    "{'. '.join(train_dataset[:10])}\n",
    "Are you ready to predict which step of a recipe the user is on, given some object states identified by a perception system?\n",
    "ASSISTANT:\n",
    "\"\"\"\n",
    "prompt_template=f'{prompt}'\n",
    "\n",
    "print(\"Output:\")\n",
    "\n",
    "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38b9355a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['At 01:00:02 PM, the possible object states for bowl are: oatmeal with raisins: 0.21, oatmeal with water: 0.2, oatmeal(dry): 0.11, empty: 0.2, oatmeal with raisins with banana: 0.09, oatmeal(cooked): 0.73, oatmeal with raisins with banana with cinnamon: 0.09. The predicted step is:',\n",
       " 'At 01:00:03 PM, the possible object states for bowl are: oatmeal with raisins: 0.29, oatmeal with water: 0.27, oatmeal(dry): 0.06, empty: 0.11, oatmeal with raisins with banana: 0.12, oatmeal(cooked): 0.37, oatmeal with raisins with banana with cinnamon: 0.04, oatmeal with raisins with banana with cinnamon with honey: 0.05. The predicted step is:',\n",
       " 'At 01:00:03 PM, the possible object states for bowl are: empty: 0.73, oatmeal with raisins with banana with cinnamon with honey: 0.18, oatmeal with raisins with banana with cinnamon: 0.09. The predicted step is:',\n",
       " 'At 01:00:04 PM, the possible object states for bowl are: oatmeal with raisins: 0.2, oatmeal with water: 0.19, oatmeal(dry): 0.03, empty: 0.55, oatmeal with raisins with banana: 0.02, oatmeal(cooked): 0.0, oatmeal with raisins with banana with cinnamon: 0.0, oatmeal with raisins with banana with cinnamon with honey: 0.0. The predicted step is:',\n",
       " 'At 01:00:04 PM, the possible object states for bowl are: empty: 0.24, oatmeal with raisins with banana with cinnamon with honey: 0.11, oatmeal with raisins with banana with cinnamon: 0.04, oatmeal(cooked): 0.33, oatmeal with water: 0.16, oatmeal with raisins with banana: 0.13, oatmeal(dry): 0.07. The predicted step is:',\n",
       " 'At 01:00:04 PM, the possible object states for mug are: dripper with filter: 0.91, dripper with filter with coffee(dry): 0.09. The predicted step is:',\n",
       " 'At 01:00:05 PM, the possible object states for bowl are: oatmeal with raisins: 0.23, oatmeal with water: 0.25, oatmeal(dry): 0.02, empty: 0.14, oatmeal with raisins with banana: 0.09, oatmeal(cooked): 0.16, oatmeal with raisins with banana with cinnamon: 0.05, oatmeal with raisins with banana with cinnamon with honey: 0.05. The predicted step is:',\n",
       " 'At 01:00:05 PM, the possible object states for bowl are: empty: 0.36, oatmeal with raisins with banana with cinnamon with honey: 0.11, oatmeal with raisins with banana with cinnamon: 0.02, oatmeal(cooked): 0.11, oatmeal with water: 0.1, oatmeal with raisins with banana: 0.15, oatmeal(dry): 0.02, oatmeal with raisins: 0.14. The predicted step is:',\n",
       " 'At 01:00:05 PM, the possible object states for mug are: dripper with filter: 0.58, dripper with filter with coffee(dry): 0.38, dripper(empty): 0.01, dripper with filter with coffee(wet): 0.03, tea-bag with water: 0.02. The predicted step is:',\n",
       " 'At 01:00:06 PM, the possible object states for bowl are: oatmeal with raisins: 0.23, oatmeal with water: 0.15, oatmeal(dry): 0.0, empty: 0.02, oatmeal with raisins with banana: 0.06, oatmeal(cooked): 0.44, oatmeal with raisins with banana with cinnamon: 0.07, oatmeal with raisins with banana with cinnamon with honey: 0.03. The predicted step is:']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_paths = ['./perception_outputs/data_oatmeal_2023.11.03-19.04.04.json']\n",
    "data = read_json(file_paths)\n",
    "test_dataset = extract_data(data, include_step=False)\n",
    "test_dataset[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce3e2f96-c331-41ef-8505-ca0312ea614a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "<s> \n",
      "USER:\n",
      "It's an oatmeal recipe and these are the steps:\n",
      "Step 1: Measure 4 Tablespoons of quick-cook oats and place in a microwave-safe bowl.\n",
      "Step 2: Add 1/2 cup water to the bowl.\n",
      "Step 3: Microwave for 30 seconds.\n",
      "Step 4: Add handful of raisins and stir.\n",
      "Step 5: Microwave for 60 more seconds then stir.\n",
      "Step 6: Top with banana slices.\n",
      "Step 7: Sprinkle with cinnamon and drizzle with honey before serving.\n",
      "\n",
      "Below you will see examples of object states identified by the perception system. Please predict the steps for them.\n",
      "At 01:00:03 PM, the possible object states for bowl are: oatmeal with raisins: 0.29, oatmeal with water: 0.27, oatmeal(dry): 0.06, empty: 0.11, oatmeal with raisins with banana: 0.12, oatmeal(cooked): 0.37, oatmeal with raisins with banana with cinnamon: 0.04, oatmeal with raisins with banana with cinnamon with honey: 0.05. The predicted step is:\n",
      "\n",
      "ASSISTANT:\n",
      "At 01:00:03 PM, the possible object states for the raisins are: raisins(dry): 0.08, raisins(cooked): 0.32, raisins(cooked with oatmeal): 0.29, raisins(cooked with oatmeal and banana): 0.15, raisins(cooked with oatmeal, banana, and cinnamon): 0.03, raisins(cooked with oatmeal, banana, cinnamon, and honey): 0.02. The predicted step is:\n",
      "\n",
      "At 01:00:03 PM, the possible object states for the water are: water(in the bowl): 0.5, water(spilled): 0.2, water(boiled): 0.1, water(evaporated): 0.18, water(not present): 0.1. The predicted step is:\n",
      "\n",
      "At 01:00:03 PM, the possible object states for the banana are: banana(whole): 0.44, banana(slices): 0.29, banana(mixed with oatmeal): 0.16, banana(mixed with oatmeal and raisins): 0.1, banana(mixed with oatmeal, raisins, and cinnamon): 0.02, banana(mixed with oatmeal, raisins, cinnamon, and honey): 0.01. The predicted step is:\n",
      "\n",
      "At 01:00:03 PM, the possible object states for the cinnamon are: cinnamon(dry): 0.08, cinnamon(mixed with oatmeal): 0.2, cinnamon(mixed with oatmeal, raisins): 0.04, cinnamon(mixed with oatmeal, raisins, and banana): 0.02, cinnamon(mixed with oatmeal, raisins, banana, and honey): \n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "USER:\n",
    "It's an oatmeal recipe and these are the steps:\n",
    "Step 1: Measure 4 Tablespoons of quick-cook oats and place in a microwave-safe bowl.\n",
    "Step 2: Add 1/2 cup water to the bowl.\n",
    "Step 3: Microwave for 30 seconds.\n",
    "Step 4: Add handful of raisins and stir.\n",
    "Step 5: Microwave for 60 more seconds then stir.\n",
    "Step 6: Top with banana slices.\n",
    "Step 7: Sprinkle with cinnamon and drizzle with honey before serving.\n",
    "\n",
    "Below you will see examples of object states identified by the perception system. Please predict the steps for them.\n",
    "{test_dataset[1]}\n",
    "\n",
    "ASSISTANT:\n",
    "\"\"\"\n",
    "prompt_template=f'{prompt}'\n",
    "\n",
    "print(\"Output:\")\n",
    "\n",
    "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9bb55d4-ab42-415b-a734-cfa9c746cc45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "<s> \n",
      "<s> [INST] <<SYS>>\n",
      "It's an oatmeal recipe and these are the steps:\n",
      "Step 1: Measure 4 Tablespoons of quick-cook oats and place in a microwave-safe bowl.\n",
      "Step 2: Add 1/2 cup water to the bowl.\n",
      "Step 3: Microwave for 30 seconds.\n",
      "Step 4: Add handful of raisins and stir.\n",
      "Step 5: Microwave for 60 more seconds then stir.\n",
      "Step 6: Top with banana slices.\n",
      "Step 7: Sprinkle with cinnamon and drizzle with honey before serving.\n",
      "\n",
      "Below you will see examples of object states identified by the perception system. Please predict the steps for them.\n",
      "At 01:00:06 PM, the possible object states for bowl are: empty: 0.72, oatmeal with raisins with banana with cinnamon with honey: 0.0, oatmeal with raisins with banana with cinnamon: 0.02, oatmeal(cooked): 0.19, oatmeal with water: 0.06, oatmeal with raisins with banana: 0.0, oatmeal(dry): 0.0, oatmeal with raisins: 0.0. The predicted step is:<</SYS>>\n",
      "\n",
      "[/INST]\n",
      "Based on the possible object states you provided, the predicted step is:\n",
      "\n",
      "Step 3: Microwave for 30 seconds.\n",
      "\n",
      "The perception system has identified the bowl as containing oatmeal with raisins and water, but not yet cooked. Therefore, the next step would be to microwave the oatmeal for 30 seconds to cook it further.</s>\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "USER:\n",
    "It's an oatmeal recipe and these are the steps:\n",
    "Step 1: Measure 4 Tablespoons of quick-cook oats and place in a microwave-safe bowl.\n",
    "Step 2: Add 1/2 cup water to the bowl.\n",
    "Step 3: Microwave for 30 seconds.\n",
    "Step 4: Add handful of raisins and stir.\n",
    "Step 5: Microwave for 60 more seconds then stir.\n",
    "Step 6: Top with banana slices.\n",
    "Step 7: Sprinkle with cinnamon and drizzle with honey before serving.\n",
    "\n",
    "Below you will see examples of object states identified by the perception system. Please predict the steps for them.\n",
    "{test_dataset[10]}<</SYS>>\n",
    "\n",
    "[/INST]\n",
    "\"\"\"\n",
    "prompt_template=f'{prompt}'\n",
    "\n",
    "print(\"Output:\")\n",
    "\n",
    "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949a0a2b-fa96-4b48-a45d-c192deb63064",
   "metadata": {},
   "source": [
    "#### Free-form Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46c82962-c0e3-40ff-9dab-e8f2d7f41dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "<s> \n",
      "<s> [INST] <<SYS>>\n",
      "When did the user start the step 1 of the recipe?<</SYS>>\n",
      "[/INST]\n",
      " As a system, I don't have the ability to track the user's actions or start times. The user may have started the step 1 of the recipe at any point in the past, and there is no information available to determine when exactly they started.\n",
      "\n",
      "However, if the user has provided any information or context about their progress in the recipe, it may be possible to infer when they started step 1 based on that information. For example, if the user has mentioned that they started the recipe yesterday, it may be reasonable to assume that they started step 1 on the previous day.\n",
      "\n",
      "Without any additional information, it is not possible to determine when the user started step 1 of the recipe with certainty.</s>\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "<s>[INST] <<SYS>>\n",
    "When did the user start the step 1 of the recipe?<</SYS>>\n",
    "[/INST]\n",
    "\"\"\"\n",
    "prompt_template=f'{prompt}'\n",
    "\n",
    "print(\"Output:\")\n",
    "\n",
    "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10095cc9-b00d-4641-bbc7-34fc89ae8383",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
