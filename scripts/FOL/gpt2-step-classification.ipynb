{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[torch]\n",
        "!pip install accelerate -U"
      ],
      "metadata": {
        "id": "RSa4ZcJ__j2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "xdp9KOPm7RYX",
        "outputId": "7ba77152-6ba7-40ac-8427-ed78a25ee7b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-2-82f31ac0027f>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  'input_ids': torch.tensor(self.tokenized_inputs['input_ids'][idx]),\n",
            "<ipython-input-2-82f31ac0027f>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  'attention_mask': torch.tensor(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='192' max='192' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [192/192 39:44, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-82f31ac0027f>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  'input_ids': torch.tensor(self.tokenized_inputs['input_ids'][idx]),\n",
            "<ipython-input-2-82f31ac0027f>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  'attention_mask': torch.tensor(\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from transformers import (\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    GPT2Tokenizer,\n",
        "    GPT2ForSequenceClassification,\n",
        ")\n",
        "import torch\n",
        "\n",
        "\n",
        "def read_json(file_paths):\n",
        "    data = []\n",
        "    for file_path in file_paths:\n",
        "        with open(file_path, 'r') as file:\n",
        "            recipe_data = json.load(file)\n",
        "            data.extend(recipe_data)\n",
        "    return data\n",
        "\n",
        "\n",
        "def extract_data(data):\n",
        "    dataset = []\n",
        "    for entry in data:\n",
        "        image_data = entry['objects']\n",
        "        step = entry['step']\n",
        "        formatted_entry = {'image_data': image_data, 'step': step}\n",
        "        dataset.append(formatted_entry)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def prepare_inputs(dataset, tokenizer):\n",
        "    texts = [f\"Image Data: {entry['image_data']}\" for entry in dataset]\n",
        "    labels = [entry['step'] - 1 for entry in dataset]\n",
        "\n",
        "    # Adding a padding token to the tokenizer\n",
        "    # tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "    # tokenizer.pad_token = '[PAD]'\n",
        "\n",
        "    # Tokenize and ensure uniform sequence length\n",
        "    max_length = 128  # Set an appropriate maximum length\n",
        "    tokenized_inputs = tokenizer(\n",
        "        texts,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    return tokenized_inputs, labels\n",
        "\n",
        "\n",
        "def prepare_dataset(tokenized_inputs, labels):\n",
        "    class CustomDataset(torch.utils.data.Dataset):\n",
        "        def __init__(self, tokenized_inputs, labels):\n",
        "            self.tokenized_inputs = tokenized_inputs\n",
        "            self.labels = labels\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            item = {\n",
        "                'input_ids': torch.tensor(self.tokenized_inputs['input_ids'][idx]),\n",
        "                'attention_mask': torch.tensor(\n",
        "                    self.tokenized_inputs['attention_mask'][idx]\n",
        "                ),\n",
        "                'labels': torch.tensor(self.labels[idx]),\n",
        "            }\n",
        "            return item\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.labels)\n",
        "\n",
        "    return CustomDataset(tokenized_inputs, labels)\n",
        "\n",
        "\n",
        "def train_model(model, train_dataset, training_args, tokenizer):\n",
        "    # Create a custom collator to handle variable-length sequences\n",
        "    def custom_collator(data):\n",
        "        input_ids = [item['input_ids'] for item in data]\n",
        "        attention_masks = [item['attention_mask'] for item in data]\n",
        "        labels = [item['labels'] for item in data]\n",
        "\n",
        "        inputs = tokenizer.pad(\n",
        "            {\"input_ids\": input_ids, \"attention_mask\": attention_masks},\n",
        "            padding='longest',\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': inputs.input_ids,\n",
        "            'attention_mask': inputs.attention_mask,\n",
        "            'labels': torch.tensor(labels),\n",
        "        }\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        data_collator=custom_collator,  # Use the custom collator\n",
        "    )\n",
        "    trainer.train()\n",
        "\n",
        "\n",
        "def main():\n",
        "    # file_paths = [\n",
        "    #     'scripts/evaluations/resource/lang_train/data_quesadilla_2023.06.16-18.53.43.json',\n",
        "    #     'scripts/evaluations/resource/lang_train/data_quesadilla_2023.06.16-18.57.48.json',\n",
        "    # ]  # Add all file paths\n",
        "    file_paths = [\"/content/data_quesadilla_2023.06.16-18.53.43.json\",\n",
        "                  \"/content/data_quesadilla_2023.06.16-18.57.48.json\"]\n",
        "    data = read_json(file_paths)\n",
        "    dataset = extract_data(data)\n",
        "\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "    tokenizer.pad_token = '[PAD]'\n",
        "    tokenized_inputs, labels = prepare_inputs(dataset, tokenizer)\n",
        "    train_dataset = prepare_dataset(tokenized_inputs, labels)\n",
        "\n",
        "    # Load model\n",
        "    model = GPT2ForSequenceClassification.from_pretrained(\n",
        "        'gpt2', pad_token_id=tokenizer.pad_token_id, num_labels=len(set(labels))\n",
        "    )\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "    # Training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='results/',\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=8,\n",
        "        save_steps=100,\n",
        "        save_total_limit=2,\n",
        "    )\n",
        "\n",
        "    # Start training\n",
        "    train_model(model, train_dataset, training_args, tokenizer)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_paths = [\"/content/data_quesadilla_2023.06.16-18.53.43.json\",\n",
        "                  \"/content/data_quesadilla_2023.06.16-18.57.48.json\"]\n",
        "data = read_json(file_paths)\n",
        "dataset = extract_data(data)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "tokenizer.pad_token = '[PAD]'\n",
        "tokenized_inputs, labels = prepare_inputs(dataset, tokenizer)\n",
        "\n",
        "train_dataset = prepare_dataset(tokenized_inputs, labels)\n",
        "\n",
        "model = GPT2ForSequenceClassification.from_pretrained(\n",
        "        '/content/results/checkpoint-100', pad_token_id=tokenizer.pad_token_id, num_labels=len(set(labels))\n",
        "    )"
      ],
      "metadata": {
        "id": "I3WL1z41ED8K"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_accuracy(predictions, labels):\n",
        "    return np.sum(predictions == labels) / len(labels)\n",
        "\n",
        "def get_predictions(model, data_loader):\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "    for batch in data_loader:\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**batch)\n",
        "        predictions.extend(np.argmax(outputs.logits.cpu().numpy(), axis=1))\n",
        "        true_labels.extend(batch['labels'].cpu().numpy())\n",
        "    return predictions, true_labels\n",
        "\n",
        "# Use DataLoader to iterate through the train_dataset in batches\n",
        "data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8)\n",
        "\n",
        "# Get predictions and true labels\n",
        "predictions, true_labels = get_predictions(model, data_loader)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = compute_accuracy(predictions, true_labels)\n",
        "print(f\"Training Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGogx41fhJJg",
        "outputId": "f57222b8-a948-4255-e8a3-5edffa0eb8cd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-82f31ac0027f>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  'input_ids': torch.tensor(self.tokenized_inputs['input_ids'][idx]),\n",
            "<ipython-input-2-82f31ac0027f>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  'attention_mask': torch.tensor(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy: 0.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# idea: to give reward to increasing step number predictions with time\n",
        "# cons: we don't know when the recipe has started\n"
      ],
      "metadata": {
        "id": "8G3Egiw-i6QA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gef4MeYwnu_4",
        "outputId": "4ef6e00b-b966-4bd2-baba-e0a55c9efb97"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 2,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 2,\n",
              " 6,\n",
              " 2,\n",
              " 6,\n",
              " 6,\n",
              " 3,\n",
              " 2,\n",
              " 3,\n",
              " 3,\n",
              " 6,\n",
              " 2,\n",
              " 6,\n",
              " 6,\n",
              " 1,\n",
              " 6,\n",
              " 2,\n",
              " 6,\n",
              " 3,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 3,\n",
              " 6,\n",
              " 2,\n",
              " 3,\n",
              " 6,\n",
              " 6,\n",
              " 2,\n",
              " 6,\n",
              " 3,\n",
              " 6,\n",
              " 6,\n",
              " 2,\n",
              " 6,\n",
              " 2,\n",
              " 6,\n",
              " 3,\n",
              " 3,\n",
              " 2,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 2,\n",
              " 3,\n",
              " 6,\n",
              " 2,\n",
              " 2,\n",
              " 6,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 6,\n",
              " 6,\n",
              " 3,\n",
              " 6,\n",
              " 2,\n",
              " 6,\n",
              " 3,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 2,\n",
              " 2,\n",
              " 3,\n",
              " 3,\n",
              " 2,\n",
              " 6,\n",
              " 6,\n",
              " 2,\n",
              " 2,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 3,\n",
              " 6,\n",
              " 6,\n",
              " 3,\n",
              " 6,\n",
              " 6,\n",
              " 3,\n",
              " 2,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 3,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 3,\n",
              " 6,\n",
              " 6,\n",
              " 3,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 2,\n",
              " 3,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 3,\n",
              " 6,\n",
              " 6,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 2,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 2,\n",
              " 3,\n",
              " 6,\n",
              " 2,\n",
              " 2,\n",
              " 6,\n",
              " 6,\n",
              " 2,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 3,\n",
              " 3,\n",
              " 2,\n",
              " 6,\n",
              " 3,\n",
              " 2,\n",
              " 2,\n",
              " 6,\n",
              " 2,\n",
              " 6,\n",
              " 3,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 3,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 3,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 3,\n",
              " 2,\n",
              " 3,\n",
              " 4,\n",
              " 6,\n",
              " 6,\n",
              " 4,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 3,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 3,\n",
              " 3,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 3,\n",
              " 3,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 3,\n",
              " 2,\n",
              " 6,\n",
              " 2,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 3,\n",
              " 6,\n",
              " 6,\n",
              " 2,\n",
              " 6,\n",
              " 6,\n",
              " 3,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 2,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 3,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 3,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 2,\n",
              " 6,\n",
              " 3,\n",
              " 6,\n",
              " 3,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 3,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 3,\n",
              " 6,\n",
              " 6,\n",
              " 3,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 3,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 3,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 3,\n",
              " 6,\n",
              " 3,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 3,\n",
              " 3,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 3,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 2,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 2,\n",
              " 3,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 3,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 2,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 2,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 3,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 3,\n",
              " 6,\n",
              " 6,\n",
              " 3,\n",
              " 6,\n",
              " 6,\n",
              " 6]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "true_labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqsKw1kLnvc1",
        "outputId": "dd342c37-86e6-47df-9aa4-05c693ff290f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 6]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ltQ7tutxoCKS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}